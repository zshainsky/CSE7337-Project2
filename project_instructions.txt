PROJECT 2 INSTRUCTIONS

******************************
******** Deliverables ********
******************************

1.	Complete code in a compressed archive (zip, tgz, etc)
2.	A readme file with complete description of used software, installation, compilation and execution instructions OR demonstrations are encouraged, but not required.
3.	A document with the results for the questions below.
4.	If you worked in a team than you need a document stating who worked on what.

******************************
************ Task ************
******************************

Develop a simplified query engine. 
Test your data only on the data in:
http://lyle.smu.edu/~fmoore

******************************
********* Questions **********
******************************

-----------------
     CANDIE
-----------------

1.	Use the web crawler you built in Project 1 that crawled a limited space, looking for text and html files. You will probably need to modify how you saved the words from the pages that you traversed to support this query engine. Describe in detail what you changed to support the saving of words for this project. [10 points]

Notes:
------
We only previously saved the term frequency and document frequency (restate format, etc.).  Add inverted index – stop words

Our Structure:  [FOR INVERTED INDEX ---> word: [[docID, termFreq], [docID, termFreq]]
	{
	 'a': [[1,2]],
	 'banana': [[3,4], [5,2]]
	 }

Also create 'docID table' which contains a unique docID, associated url, and first twenty words of the document.

*********************
ANSWER TO QUESTION 1:
*********************
Previously in project one, we saved our corpus of words in a text file along with the total number of times a word appeared across all documents, and the total number of documents in which the term appeared.  See illustration below:

  ## PROJECT 1 OUTPUT ##
	'Spring', [7, 4]
	'at', [7, 4]
	'for', [6, 6]
	'Web', [6, 5]
	'CSE', [6, 3]
	'Moore', [6, 3]


For project two, we have taken it a step further by capturing the word in an inverted index with each unique word having a posting list identifying the document id in which a term appeared, and how many times it appeared in that particular document.  See illustration below:

  ## PROJECT 2 OUTPUT ##
 // PLACE EXAMPLE HERE

 As you can see, we also are using unique document id's to reference each separate document.  These are tracked within a collections index bearing the unique id, the associated url, and the first twenty words captured from the scrape.  See illustration below:

   ## COLLECTIONS INDEX ##
 // PLACE EXAMPLE HERE


Both the collections index and the inverted index are held in python dictionaries, with each term in the inverted index serving as the key, and the associated value to that key is a set object containing the list of unique document id's where that key is found, and the frequency with respect to that key.  We chose to use a set here because it forces only unique keys.

**********************

2.	You will need a dictionary of words.  What is your definition of “word”? Will you generate the dictionary while navigating the pages or as a separate step? Explain your approach. If needed, you can define a fixed size for the dictionary based upon the results from Project1, and allowing for a 10% increase. [20 points]

*********************
ANSWER TO QUESTION 2:
*********************

Because we are using Python, we do not need to decide on a fixed size data structure.  Python handles dynamic sizing quite well.  Before building our index, we save the first twenty words from each page into the collections index.  To construct our inverted index, we decided to first remove symbols and case from our text yielded by the scraping mechanism.  Subsequently, we decided to remove many of the more frequently used, but meaningless words found on the internet today, such as and, the, is, be, etc.  Our stop words are held in a text file in the directory and and loaded into a set upon initializing an inverted index model.  After we removed the stop words, we ran our final list through the porterStemmer to stem the words to the roots.

*********************


3.	For purposes of this project, you may assume a maximum of 20 documents. You will need to create a term frequency matrix for all documents. [20 points]

Notes:
------
Term Frequency matrix
docID			TERM1 	TERM2	TERM3
11	     	     0        0       2 
12            	 1        4       0

---------------------------------------------
|  Stopped here because I was not sure if   |
|  you (Zack) had already done this.  This  |
|  is not going to be difficult to implement|
|  at all and we can build a method to do   |
|  so quite easily I believe.  Let me know  |
|  if this is supposed to have tf-idf. Tks  |
---------------------------------------------


*********************
ANSWER TO QUESTION 3:
*********************

     =P  =P  =P

*********************

-----------------
      ZACK
-----------------

4.	The user should be able to enter multiple queries, consisting of one or more query words separated by space.  What happens if a user enters a word that is not in the dictionary? [10 points]


*********************
ANSWER TO QUESTION 4:
*********************

 The words that are entered by the user that are not found in our index of words will be removed from the list of query terms and ignored. The words that are not found will be outputted to the user with this message: “These words were not found and removed from the query:  ['test’]”. This message indicates that the words ‘test’ was not found in our word index. If no words from the user’s search were found in any of the documents than the user is alerted with this message: “No words from your search were found in any documents... Please try new search terms!”, and prompted to enter a new search.

*********************
5.	Compute the cosine similarity of the query against all documents. Display the measure and document URL in descending numerical order for the top N results. What value of N would you pick?  GRADUATE STUDENTS: Also include in the display, the first 20 words of the document. If you stemmed the words, this can be the stemmed version. [40 points]


*********************
ANSWER TO QUESTION 5:
*********************

  We have decided to display up to 10 results as our N value. The relevant documents will be displayed in descending order starting with the most relevant (highest cosine similarity). We have those N to be 10 because this is around the standard for all of the industry leading search engines, Google, Yahoo, Bing. Excluding advertisements, all three of these search engines output between 9 and 11 links to articles on each page of every search.

*********************





